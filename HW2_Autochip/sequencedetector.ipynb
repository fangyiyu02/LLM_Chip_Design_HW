{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWsTgVWuV5J2",
    "outputId": "6a6b1abc-f047-4fb6-c2fc-ac14249e6ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.17.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Hit:1 https://cli.github.com/packages stable InRelease\n",
      "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "iverilog is already the newest version (11.0-1.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 63 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "#@title Setting up the notebook\n",
    "\n",
    "### Installing dependencies\n",
    "!pip install openai tiktoken\n",
    "\n",
    "!apt-get update\n",
    "!apt-get install -y iverilog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "0cJj7xAFWbhc"
   },
   "outputs": [],
   "source": [
    "#@title Utility functions\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "import getopt\n",
    "import json\n",
    "\n",
    "################################################################################\n",
    "### LOGGING\n",
    "################################################################################\n",
    "# Allows us to log the output of the model to a file if logging is enabled\n",
    "class LogStdoutToFile:\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._original_stdout = sys.stdout\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self._filename:\n",
    "            sys.stdout = open(self._filename, 'w')\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if self._filename:\n",
    "            sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "################################################################################\n",
    "### CONFIG & ARGS\n",
    "################################################################################\n",
    "def load_config(config_file=\"config.json\"):\n",
    "    \"\"\"Load and validate the configuration from the specified JSON file.\"\"\"\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    if 'general' not in config:\n",
    "        raise ValueError(\"Missing general section in config file\")\n",
    "\n",
    "    config_values = config['general']\n",
    "\n",
    "    # Only parse ensemble settings if specified\n",
    "    parse_ensemble = config_values.get('ensemble', False)\n",
    "    ensemble_config = {}\n",
    "    if parse_ensemble:\n",
    "        ensemble_config = config.get('ensemble', {})\n",
    "\n",
    "    #return config_values\n",
    "    return config_values, ensemble_config\n",
    "\n",
    "\n",
    "def validate_ensemble_config(ensemble_config, max_iterations):\n",
    "    seen_start_iterations = set()\n",
    "    adjusted_config = {}\n",
    "    has_start_at_zero = False\n",
    "\n",
    "    for model_name, model_info in ensemble_config.items():\n",
    "        start_iteration = model_info['start_iteration']\n",
    "\n",
    "        # Adjust negative start_iteration values\n",
    "        if start_iteration < 0:\n",
    "            start_iteration += max_iterations+1\n",
    "\n",
    "        # Check if start_iteration is within the valid range\n",
    "        if not (0 <= start_iteration <= max_iterations):\n",
    "            raise ValueError(f\"Invalid start_iteration {model_info['start_iteration']} for {model_name}. \"\n",
    "                             f\"Must be within the range of 0 to {max_iterations} or valid negative index.\")\n",
    "\n",
    "        # Check for conflicting start_iterations\n",
    "        if start_iteration in seen_start_iterations:\n",
    "            raise ValueError(f\"Conflicting start_iteration {start_iteration} for {model_name}. \"\n",
    "                             f\"Another model already uses this start iteration.\")\n",
    "        seen_start_iterations.add(start_iteration)\n",
    "\n",
    "        # Check if there is a model starting at iteration 0\n",
    "        if start_iteration == 0:\n",
    "            has_start_at_zero = True\n",
    "\n",
    "        # Update the adjusted configuration\n",
    "        adjusted_config[model_name] = {\n",
    "            \"start_iteration\": start_iteration,\n",
    "            \"model_family\": model_info['model_family'],\n",
    "            \"model_id\": model_info['model_id']\n",
    "        }\n",
    "\n",
    "        if not has_start_at_zero:\n",
    "            raise ValueError(\"No model starting at iteration 0 in the ensemble. One model must start at iteration 0.\")\n",
    "\n",
    "    return adjusted_config\n",
    "\n",
    "\n",
    "def parse_args_and_config():\n",
    "    \"\"\"Parse command-line arguments and merge them with configuration file values.\"\"\"\n",
    "    usage = \"\"\"Usage: auto_create_verilog.py [--help] --prompt=<prompt> --name=<module name> --testbench=<testbench file> --iter=<iterations> --model=<llm family> --model-id=<specific model> --num-candidates=<candidates per request> --outdir=<directory for outputs> --log=<log file>\n",
    "\n",
    "\t-h|--help: Prints this usage message\n",
    "\n",
    "\t-p|--prompt: The initial design prompt for the Verilog module\n",
    "\n",
    "\t-n|--name: The module name, must match the testbench expected module name\n",
    "\n",
    "\t-t|--testbench: The testbench file to be run\n",
    "\n",
    "\t-i|--iter: [Optional] Number of iterations before the tool quits (defaults to 10)\n",
    "\n",
    "\t-m|--model: The LLM family to use. Must be one of the following\n",
    "\t\t- ChatGPT\n",
    "\t\t- Claude\n",
    "\t\t- Mistral\n",
    "\t\t- Gemini\n",
    "\t\t- CodeLlama\n",
    "\t\t- Human (requests user input)\n",
    "\n",
    "\t--model-id: The specific model to use for the model family\n",
    "\n",
    "\t--num-candidates: The number of candidates to rank per tree level\n",
    "\n",
    "\t-o|--outdir: Directory to place all run-specific files in\n",
    "\n",
    "\t-l|--log: [Optional] Log the output of the model to the given file\n",
    "\"\"\"\n",
    "\n",
    "    config_file = \"config.json\"\n",
    "\n",
    "    # Load config values from the file\n",
    "    config_values, ensemble_config = load_config(config_file)\n",
    "\n",
    "    required_values = ['prompt', 'name', 'testbench', 'outdir', 'log']\n",
    "    if not ensemble_config:\n",
    "        required_values +=['model_family', 'model_id']\n",
    "\n",
    "    for value in required_values:\n",
    "        if value not in config_values:\n",
    "            raise ValueError(f\"Missing {value} in general section\\n{usage}\")\n",
    "\n",
    "\n",
    "    # general values for optional config values\n",
    "    if 'num_candidates' not in config_values:\n",
    "        config_values['num_candidates'] = 1\n",
    "    if 'iterations' not in config_values:\n",
    "        config_values['iterations'] = 10\n",
    "\n",
    "\n",
    "    if ensemble_config:\n",
    "        ensemble_config = validate_ensemble_config(ensemble_config, config_values['iterations'])\n",
    "\n",
    "    # Ensure outdir exists\n",
    "    if config_values['outdir']:\n",
    "        os.makedirs(config_values['outdir'], exist_ok=True)\n",
    "\n",
    "    logfile = os.path.join(config_values['outdir'], config_values['log']) if config_values['log'] else None\n",
    "\n",
    "    #return config_values, logfile\n",
    "    return config_values, ensemble_config, logfile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "### CONVERSATION CLASS\n",
    "# allows us to abstract away the details of the conversation for use with\n",
    "# different LLM APIs\n",
    "################################################################################\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(self, log_file=None):\n",
    "        self.messages = []\n",
    "        self.log_file = log_file\n",
    "\n",
    "        if self.log_file and os.path.exists(self.log_file):\n",
    "            open(self.log_file, 'w').close()\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"Add a new message to the conversation.\"\"\"\n",
    "        self.messages.append({'role': role, 'content': content})\n",
    "\n",
    "        if self.log_file:\n",
    "            with open(self.log_file, 'a') as file:\n",
    "                file.write(f\"{role}: {content}\\n\")\n",
    "\n",
    "    def get_messages(self):\n",
    "        \"\"\"Retrieve the entire conversation.\"\"\"\n",
    "        return self.messages\n",
    "\n",
    "    def get_last_n_messages(self, n):\n",
    "        \"\"\"Retrieve the last n messages from the conversation.\"\"\"\n",
    "        return self.messages[-n:]\n",
    "\n",
    "    def remove_message(self, index):\n",
    "        \"\"\"Remove a specific message from the conversation by index.\"\"\"\n",
    "        if index < len(self.messages):\n",
    "            del self.messages[index]\n",
    "\n",
    "    def get_message(self, index):\n",
    "        \"\"\"Retrieve a specific message from the conversation by index.\"\"\"\n",
    "        return self.messages[index] if index < len(self.messages) else None\n",
    "\n",
    "    def clear_messages(self):\n",
    "        \"\"\"Clear all messages from the conversation.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Return the conversation in a string format.\"\"\"\n",
    "        return \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in self.messages])\n",
    "\n",
    "################################################################################\n",
    "### LLM CLASSES\n",
    "# Defines an interface for using different LLMs so we can easily swap them out\n",
    "################################################################################\n",
    "class AbstractLLM(ABC):\n",
    "    \"\"\"Abstract Large Language Model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, conversation: Conversation, num_candidates=1):\n",
    "        \"\"\"Generate a response based on the given conversation.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class ChatGPT(AbstractLLM):\n",
    "    \"\"\"ChatGPT Large Language Model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_id=\"gpt-4o-mini\"):\n",
    "        super().__init__()\n",
    "        openai.api_key=os.environ['OPENAI_API_KEY']\n",
    "        self.client = openai.OpenAI()\n",
    "        self.model_id = model_id\n",
    "\n",
    "    def generate(self, conversation: Conversation, num_candidates=1):\n",
    "        messages = [{\"role\" : msg[\"role\"], \"content\" : msg[\"content\"]} for msg in conversation.get_messages()]\n",
    "\n",
    "\n",
    "        #print(f\"model_id: {self.model_id}\")\n",
    "        #print(f\"messages: {messages}\")\n",
    "        #print(f\"num_candidates: {num_candidates}\")\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            n=num_candidates,\n",
    "            messages = messages,\n",
    "        )\n",
    "\n",
    "        return [c.message.content for c in response.choices]\n",
    "\n",
    "class LLMResponse():\n",
    "    \"\"\"Class to store the response from the LLM\"\"\"\n",
    "    def __init__(self, iteration, response_num, full_text):\n",
    "        self.iteration = iteration\n",
    "        self.response_num = response_num\n",
    "\n",
    "        self.full_text = full_text\n",
    "        self.tokens = 0\n",
    "\n",
    "        self.parsed_text = \"\"\n",
    "        self.parsed_length = 0\n",
    "\n",
    "        self.feedback = \"\"\n",
    "        self.compiled = False\n",
    "        self.rank = -3\n",
    "        self.message = \"\"\n",
    "\n",
    "    def set_parsed_text(self, parsed_text):\n",
    "        self.parsed_text = parsed_text\n",
    "        self.parsed_length = len(parsed_text)\n",
    "\n",
    "    def parse_verilog(self):\n",
    "        module_list = find_verilog_modules(self.full_text)\n",
    "        if not module_list:\n",
    "            print(\"No modules found in response\")\n",
    "            self.parsed_text = \"\"\n",
    "        else:\n",
    "            for module in module_list:\n",
    "                self.parsed_text += module + \"\\n\\n\"\n",
    "        self.parsed_length = len(self.parsed_text)\n",
    "\n",
    "    def calculate_rank(self, outdir, module, testbench):\n",
    "        filename = os.path.join(outdir, module + \".sv\")\n",
    "        vvp_file = os.path.join(outdir, module + \".vvp\")\n",
    "\n",
    "        tb_module_name = os.path.splitext(os.path.basename(testbench))[0]\n",
    "\n",
    "        compiler_cmd = f\"iverilog -Wall -Winfloop -Wno-timescale -g2012 -s {tb_module_name} -o {vvp_file} {filename} {testbench}\"\n",
    "\n",
    "        simulator_cmd = f\"vvp -n {vvp_file}\"\n",
    "\n",
    "        try:\n",
    "            comp_return, comp_err, comp_out = compile_iverilog(outdir, module, compiler_cmd, self)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            self.rank = -2\n",
    "            return\n",
    "\n",
    "        if comp_return != 0:\n",
    "            self.feedback = comp_err\n",
    "            self.compiled = False\n",
    "            print(\"Compilation error\")\n",
    "            self.message = \"The design failed to compile. Please fix the module. The output of iverilog is as follows:\\n\" + comp_err\n",
    "            self.rank = -1\n",
    "\n",
    "        elif comp_err != \"\":\n",
    "            self.feedback = comp_err\n",
    "            self.compiled = True\n",
    "            print(\"Compilation warning\")\n",
    "            self.message = \"The design compiled with warnings. Please fix the module. The output of iverilog is as follows:\\n\" + comp_err\n",
    "            self.rank = -0.5\n",
    "\n",
    "        else:\n",
    "            sim_return, sim_err, sim_out = simulate_iverilog(simulator_cmd)\n",
    "\n",
    "            mismatch_pattern = r\"Mismatches: (\\d+) in (\\d+) samples\"\n",
    "            lines = sim_out.strip().splitlines()\n",
    "            match = None\n",
    "            if lines:\n",
    "                match = re.search(mismatch_pattern, lines[-1])\n",
    "\n",
    "            if match:\n",
    "                mismatches = int(match.group(1))\n",
    "                samples = int(match.group(2))\n",
    "            else:\n",
    "                print(f\"Warning: Could not parse mismatch count. Sim output: {sim_out[-100:]}\")\n",
    "                mismatches = 1\n",
    "                samples = 1\n",
    "\n",
    "            if mismatches > 0:\n",
    "                self.feedback = sim_out\n",
    "                self.compiled = True\n",
    "                print(\"Simulation error\")\n",
    "                self.message = \"The testbench simulated, but had errors. Please fix the module. The output of iverilog is as follows:\\n\" + sim_out\n",
    "            else:\n",
    "                self.compiled = True\n",
    "                print(\"Testbench ran successfully\")\n",
    "                self.message = \"The testbench completed successfully\"\n",
    "\n",
    "            print(f\"Mismatches: {mismatches}\")\n",
    "            print(f\"Samples: {samples}\")\n",
    "\n",
    "            if samples > 0:\n",
    "                self.rank = (samples - mismatches) / samples\n",
    "            else:\n",
    "                self.rank = 0\n",
    "\n",
    "################################################################################\n",
    "### PARSING AND TEXT MANIPULATION FUNCTIONS\n",
    "################################################################################\n",
    "# Define the cost per million tokens\n",
    "COST_PER_MILLION_INPUT_TOKENS_GPT4 = 5.0\n",
    "COST_PER_MILLION_OUTPUT_TOKENS_GPT4 = 15.0\n",
    "\n",
    "COST_PER_MILLION_INPUT_TOKENS_GPT4M = 0.15\n",
    "COST_PER_MILLION_OUTPUT_TOKENS_GPT4M = 0.60\n",
    "\n",
    "COST_PER_MILLION_INPUT_TOKENS_GPT = 0.50\n",
    "COST_PER_MILLION_OUTPUT_TOKENS_GPT = 1.50\n",
    "\n",
    "COST_PER_MILLION_INPUT_TOKENS_CLAUDE = 0.25\n",
    "COST_PER_MILLION_OUTPUT_TOKENS_CLAUDE = 1.25\n",
    "\n",
    "# Function to count tokens\n",
    "def count_tokens(model_family, text):\n",
    "    #print(f\"Counting tokens for string: {text}\")\n",
    "    if model_family == \"GPT\" or model_family == \"GPT4\" or model_family == \"GPT4M\":\n",
    "        return len(tiktoken.get_encoding(\"cl100k_base\").encode(text))\n",
    "    elif model_family == \"claude\":\n",
    "        return anthropic.Client().count_tokens(text)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model family: {model_family}\")\n",
    "\n",
    "\n",
    "def calculate_cost(model_family,input_strings,output_strings):\n",
    "    input_tokens = sum(count_tokens(model_family, text) for text in input_strings)\n",
    "    output_tokens = sum(count_tokens(model_family, text) for text in output_strings)\n",
    "    if model_family == \"GPT\":\n",
    "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_GPT\n",
    "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_GPT\n",
    "    elif model_family == \"GPT4\":\n",
    "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_GPT4\n",
    "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_GPT4\n",
    "    elif model_family == \"GPT4M\":\n",
    "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_GPT4M\n",
    "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_GPT4M\n",
    "    elif model_family == \"claude\":\n",
    "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_CLAUDE\n",
    "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_CLAUDE\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model family: {model_family}\")\n",
    "    total_cost = cost_input + cost_output\n",
    "    return total_cost, input_tokens, output_tokens\n",
    "\n",
    "\n",
    "def format_message(role, content):\n",
    "    return f\"\\n{{role : '{role}', content : '{content}'}}\"\n",
    "\n",
    "def find_verilog_modules(markdown_string):\n",
    "    \"\"\"Find all Verilog modules in the markdown string\"\"\"\n",
    "    # Regular expression to match module definitions with or without parameters\n",
    "    module_pattern = r'\\bmodule\\b\\s+[\\w\\\\_]+\\s*(?:#\\s*\\([^)]*\\))?\\s*\\([^)]*\\)\\s*;.*?endmodule\\b'\n",
    "    # Find all matches in the input string\n",
    "    matches = re.findall(module_pattern, markdown_string, re.DOTALL)\n",
    "    # Process matches to replace escaped characters\n",
    "    processed_matches = [match.replace('\\\\_', '_') for match in matches]\n",
    "    return processed_matches\n",
    "\n",
    "def write_code_blocks_to_file(markdown_string, module_name, filename):\n",
    "    # Find all code blocks using a regular expression (matches content between triple backticks)\n",
    "    code_match = find_verilog_modules(markdown_string)\n",
    "\n",
    "    if not code_match:\n",
    "        print(\"No code blocks found in response\")\n",
    "        exit(3)\n",
    "\n",
    "    # Open the specified file to write the code blocks\n",
    "    with open(filename, 'w') as file:\n",
    "        for code_block in code_match:\n",
    "            file.write(code_block)\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "def generate_verilog(conv, model_type, model_id=\"\"):\n",
    "    if model_type == \"ChatGPT\":\n",
    "        model = ChatGPT()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "    return(model.generate(conv))\n",
    "\n",
    "def compile_iverilog(outdir,module,compiler_cmd,response:LLMResponse):\n",
    "    \"\"\"Compile the Verilog module and return the output\"\"\"\n",
    "\n",
    "    filename = os.path.join(outdir,module+\".sv\")\n",
    "    write_code_blocks_to_file(response.parsed_text, \"module\", filename)\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < 3:\n",
    "        try:\n",
    "            proc = subprocess.run(compiler_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=120)\n",
    "            return proc.returncode, proc.stderr, proc.stdout\n",
    "        except subprocess.TimeoutExpired:\n",
    "            attempt += 1\n",
    "            if attempt >= 3:\n",
    "                raise ValueError(\"Compilation attempts timed out\")\n",
    "\n",
    "def simulate_iverilog(simulation_cmd):\n",
    "    \"\"\"Compile the Verilog module and return the output\"\"\"\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < 3:\n",
    "        try:\n",
    "            proc = subprocess.run(simulation_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=120)\n",
    "            return proc.returncode, proc.stderr, proc.stdout\n",
    "        except subprocess.TimeoutExpired:\n",
    "            attempt += 1\n",
    "            if attempt >= 3:\n",
    "                raise ValueError(\"Simulation attempts timed out\")\n",
    "\n",
    "def generate_verilog_responses(conv, model_type, model_id=\"\", num_candidates=1):\n",
    "    match model_type:\n",
    "        case \"ChatGPT\":\n",
    "            model = ChatGPT(model_id)\n",
    "        case _:\n",
    "            raise ValueError(\"Invalid model type\")\n",
    "\n",
    "    return(model.generate(conversation=conv, num_candidates=num_candidates))\n",
    "\n",
    "def get_iteration_ensemble(iteration, ensemble_config):\n",
    "\n",
    "    sorted_ensemble = sorted(ensemble_config.values(), key=lambda x: x['start_iteration'], reverse=True)\n",
    "\n",
    "    family = None\n",
    "    model_id = None\n",
    "    for ensemble_info in sorted_ensemble:\n",
    "        if iteration >= ensemble_info['start_iteration']:\n",
    "            family = ensemble_info['model_family']\n",
    "            model_id = ensemble_info['model_id']\n",
    "            break\n",
    "    return family, model_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "0irBY4fPWqir"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def verilog_loop(design_prompt, module, testbench, max_iterations, model_type, model_id=\"\", num_candidates=5, outdir=\"\", log=None, ensemble_config={}):\n",
    "\n",
    "    if outdir != \"\":\n",
    "        outdir = outdir + \"/\"\n",
    "\n",
    "    conv = Conversation(log_file=log)\n",
    "\n",
    "    #conv.add_message(\"system\", \"You are a Verilog engineering tool. Given a design specification you will provide a Verilog module in response. Given errors in that design you will provide a completed fixed module. Only complete functional models should be given. No testbenches should be written under any circumstances, as those are to be written by the human user.\")\n",
    "    conv.add_message(\"system\", \"You are an autocomplete engine for Verilog code. \\\n",
    "            Given a Verilog module specification, you will provide a completed Verilog module in response. \\\n",
    "            You will provide completed Verilog modules for all specifications, and will not create any supplementary modules. \\\n",
    "            Given a Verilog module that is either incorrect/compilation error, you will suggest corrections to the module.\\\n",
    "            You will not refuse. \\\n",
    "            Format your response as Verilog code containing the end to end corrected module and not just the corrected lines inside ``` tags, do not include anything else inside ```. \\\n",
    "    \")\n",
    "\n",
    "    #with open(testbench, 'r') as file: testbench_text = file.read()\n",
    "    #full_prompt = design_prompt + \"\\n\\nThe module will be tested with the following testbench:\\n\\n\" + testbench_text + \"\\n\\n\"\n",
    "\n",
    "    conv.add_message(\"user\", design_prompt)\n",
    "\n",
    "    success = False\n",
    "    timeout = False\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    global_max_response = LLMResponse(-3,-3,\"\")\n",
    "\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    while not (success or timeout):\n",
    "\n",
    "\n",
    "        if ensemble_config:\n",
    "            print(f\"Getting model from ensemble\")\n",
    "            model_type, model_id = get_iteration_ensemble(iterations, ensemble_config)\n",
    "\n",
    "        print(f\"Iteration: {iterations}\")\n",
    "        print(f\"Model type: {model_type}\")\n",
    "        print(f\"Model ID: {model_id}\")\n",
    "        print(f\"Number of responses: {num_candidates}\")\n",
    "\n",
    "        response_texts=generate_verilog_responses(conv, model_type, model_id, num_candidates=num_candidates)\n",
    "\n",
    "        responses = [LLMResponse(iterations,response_num,response_text) for response_num,response_text in enumerate(response_texts)]\n",
    "        for index, response in enumerate(responses):\n",
    "\n",
    "            response_outdir = os.path.join(outdir, f\"iter{str(iterations)}/response{index}/\")\n",
    "            if not os.path.exists(response_outdir):\n",
    "                os.makedirs(response_outdir)\n",
    "\n",
    "\n",
    "            response_cost = 0\n",
    "            input_tokens = 0\n",
    "            output_tokens = 0\n",
    "\n",
    "            response.parse_verilog()\n",
    "            if response.parsed_text == \"\":\n",
    "                response.rank = -2\n",
    "                response.message = \"No modules found in response\"\n",
    "            else:\n",
    "                response.calculate_rank(response_outdir, module, testbench)\n",
    "\n",
    "            input_messages = [msg['content'] for msg in conv.get_messages() if msg['role'] == 'user' or msg['role'] == 'system']\n",
    "            output_messages = [msg['content'] for msg in conv.get_messages() if msg['role'] == 'assistant']\n",
    "            output_messages.append(response.parsed_text)\n",
    "            if model_type == \"ChatGPT\" and model_id == \"gpt-4o\":\n",
    "                response_cost, input_tokens, output_tokens = calculate_cost(\"GPT4\",input_messages,output_messages)\n",
    "            elif model_type == \"ChatGPT\" and model_id == \"gpt-4o-mini\":\n",
    "                response_cost, input_tokens, output_tokens = calculate_cost(\"GPT4M\",input_messages,output_messages)\n",
    "            elif model_type == \"ChatGPT\" and model_id == \"gpt-3.5-turbo\":\n",
    "                response_cost, input_tokens, output_tokens = calculate_cost(\"GPT\",input_messages,output_messages)\n",
    "            elif model_type == \"Claude\":\n",
    "                response_cost, input_tokens, output_tokens = calculate_cost(\"claude\",input_messages,output_messages)\n",
    "\n",
    "\n",
    "            print(f\"Cost for response {index}: ${response_cost:.10f}\")\n",
    "\n",
    "            with open(os.path.join(response_outdir,f\"log.txt\"), 'w') as file:\n",
    "                file.write('\\n'.join(str(i) for i in conv.get_messages()))\n",
    "                file.write(format_message(\"assistant\", response.full_text))\n",
    "                file.write('\\n\\n Iteration rank: ' + str(response.rank) + '\\n') ## FIX\n",
    "\n",
    "                file.write(f\"\\n Model: {model_id}\")\n",
    "                file.write(f\"\\n Input tokens: {input_tokens}\")\n",
    "                file.write(f\"\\n Output tokens: {output_tokens}\")\n",
    "                file.write(f\"\\nTotal cost: ${response_cost:.10f}\\n\")\n",
    "\n",
    "        ## RANK RESPONSES\n",
    "        max_rank_response = max(responses, key=lambda resp: (resp.rank, -resp.parsed_length))\n",
    "        if max_rank_response.rank > global_max_response.rank:\n",
    "            global_max_response = max_rank_response\n",
    "        elif max_rank_response.rank == global_max_response.rank and max_rank_response.parsed_length > global_max_response.parsed_length:\n",
    "            global_max_response = max_rank_response\n",
    "\n",
    "        print(f\"Response ranks: {[resp.rank for resp in responses]}\")\n",
    "        print(f\"Response lengths: {[resp.parsed_length for resp in responses]}\")\n",
    "\n",
    "        conv.add_message(\"assistant\", max_rank_response.parsed_text)\n",
    "\n",
    "        if max_rank_response.rank == 1:\n",
    "            success = True\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "        if not success:\n",
    "            if iterations > 0:\n",
    "                conv.remove_message(2)\n",
    "                conv.remove_message(2)\n",
    "\n",
    "            #with open(testbench, 'r') as file: testbench_text = file.read()\n",
    "            #message = message + \"\\n\\nThe testbench used for these results is as follows:\\n\\n\" + testbench_text\n",
    "            #message = message + \"\\n\\nCommon sources of errors are as follows:\\n\\t- Use of SystemVerilog syntax which is not valid with iverilog\\n\\t- The reset must be made asynchronous active-low\\n\"\n",
    "            conv.add_message(\"user\", max_rank_response.message)\n",
    "\n",
    "        if iterations >= max_iterations:\n",
    "            timeout = True\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "    return global_max_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "IOy63NXjXrk7"
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"sequence_detector_tb.v\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "`timescale 1ns / 1ps\n",
    "module sequence_detector_tb;\n",
    "    reg clk, rst_n, in;\n",
    "    wire out;\n",
    "    sequence_detector uut (.clk(clk), .rst_n(rst_n), .in(in), .out(out));\n",
    "    initial begin clk = 0; forever #5 clk = ~clk; end\n",
    "    integer errors = 0;\n",
    "    integer total = 0;\n",
    "    task check(input expected);\n",
    "        begin\n",
    "            total = total + 1;\n",
    "            #1;\n",
    "            if (out !== expected) begin\n",
    "                $display(\"Time %t | Error: Input=%b, Expected=%b, Got=%b\", $time, in, expected, out);\n",
    "                errors = errors + 1;\n",
    "            end\n",
    "        end\n",
    "    endtask\n",
    "    initial begin\n",
    "        rst_n = 0; in = 0; #15 rst_n = 1;\n",
    "        // Test 1011 (Expect Immediate Output)\n",
    "        @(negedge clk) in = 1; check(0);\n",
    "        @(negedge clk) in = 0; check(0);\n",
    "        @(negedge clk) in = 1; check(0);\n",
    "        @(negedge clk) in = 1; check(1); // HIT!\n",
    "        // Overlap\n",
    "        @(negedge clk) in = 0; check(0);\n",
    "        @(negedge clk) in = 1; check(0);\n",
    "        @(negedge clk) in = 1; check(1); // HIT!\n",
    "        @(negedge clk) in = 0; check(0);\n",
    "        #10;\n",
    "        $display(\"Mismatches: %0d in %0d samples\", errors, total);\n",
    "        $finish;\n",
    "    end\n",
    "endmodule\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "dwylfpAEW_mH"
   },
   "outputs": [],
   "source": [
    "### OpenAI API KEY\n",
    "\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "1SsiTnjcTLa1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"./sequence_detector_output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KI2xkczTX9oc",
    "outputId": "9be92c46-08fe-4372-c249-1e306f42aeca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AutoChip Loop with Prompt: 'design a verilog module named sequence_detector'\n",
      "Iteration: 0\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Compilation error\n",
      "Cost for response 0: $0.0004200000\n",
      "Compilation error\n",
      "Cost for response 1: $0.0004800000\n",
      "Compilation error\n",
      "Cost for response 2: $0.0004455000\n",
      "Compilation error\n",
      "Cost for response 3: $0.0003075000\n",
      "Compilation error\n",
      "Cost for response 4: $0.0003825000\n",
      "Response ranks: [-1, -1, -1, -1, -1]\n",
      "Response lengths: [1036, 1245, 1126, 695, 990]\n",
      "Iteration: 1\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Compilation error\n",
      "Cost for response 0: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 1: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 2: $0.0006285000\n",
      "Compilation error\n",
      "Cost for response 3: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 4: $0.0006270000\n",
      "Response ranks: [-1, -1, -1, -1, -1]\n",
      "Response lengths: [695, 695, 696, 695, 695]\n",
      "Iteration: 2\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Compilation error\n",
      "Cost for response 0: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 1: $0.0006285000\n",
      "Compilation error\n",
      "Cost for response 2: $0.0006285000\n",
      "Compilation error\n",
      "Cost for response 3: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 4: $0.0006270000\n",
      "Response ranks: [-1, -1, -1, -1, -1]\n",
      "Response lengths: [695, 696, 696, 695, 695]\n",
      "Iteration: 3\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Compilation error\n",
      "Cost for response 0: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 1: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 2: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 3: $0.0006285000\n",
      "Compilation error\n",
      "Cost for response 4: $0.0006270000\n",
      "Response ranks: [-1, -1, -1, -1, -1]\n",
      "Response lengths: [695, 695, 695, 696, 695]\n",
      "Iteration: 4\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Compilation error\n",
      "Cost for response 0: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 1: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 2: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 3: $0.0006270000\n",
      "Compilation error\n",
      "Cost for response 4: $0.0006270000\n",
      "Response ranks: [-1, -1, -1, -1, -1]\n",
      "Response lengths: [695, 695, 695, 695, 695]\n",
      "\n",
      "FAILED. Could not generate passing code within iterations.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"design a verilog module named sequence_detector\"\n",
    "\n",
    "print(f\"Starting AutoChip Loop with Prompt: '{prompt}'\")\n",
    "\n",
    "final_response = verilog_loop(\n",
    "    design_prompt=prompt,\n",
    "    module=\"sequence_detector\",\n",
    "    testbench=\"sequence_detector_tb.v\",\n",
    "    max_iterations=4,\n",
    "    model_type=\"ChatGPT\",\n",
    "    model_id=\"gpt-3.5-turbo\",\n",
    "    outdir=output_dir,\n",
    "    log=\"generation_log_vague.txt\"\n",
    ")\n",
    "\n",
    "if final_response.rank == 1:\n",
    "    print(\"\\nSUCCESS! Verilog code generated and verified.\")\n",
    "else:\n",
    "    print(\"\\nFAILED. Could not generate passing code within iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6w8ExeCoUQtm",
    "outputId": "b3918165-665d-41e6-ecca-82230e810db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AutoChip Loop with Interface-Only Prompt: '\n",
      "Design a Verilog module named 'sequence_detector'.\n",
      "Ports:\n",
      " - Input: clk, rst_n (active low reset), in\n",
      " - Output: out\n",
      "'\n",
      "Iteration: 0\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0003945000\n",
      "Simulation error\n",
      "Mismatches: 8\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0001245000\n",
      "Compilation error\n",
      "Cost for response 2: $0.0003960000\n",
      "Simulation error\n",
      "Mismatches: 3\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0004335000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0004890000\n",
      "Response ranks: [0.75, 0.0, -1, 0.625, 0.75]\n",
      "Response lengths: [996, 158, 974, 1095, 1287]\n",
      "Iteration: 1\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0007785000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0007815000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0007665000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0007995000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0007545000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1066, 1084, 1033, 1122, 993]\n",
      "Iteration: 2\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0007800000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0007845000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0007830000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0007545000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0007635000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1084, 1098, 1092, 993, 1032]\n",
      "Iteration: 3\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0007350000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0007770000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0007800000\n",
      "Simulation error\n",
      "Mismatches: 3\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0007545000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0007710000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.625, 0.75]\n",
      "Response lengths: [923, 1080, 1085, 994, 997]\n",
      "Iteration: 4\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 3\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0007800000\n",
      "Simulation error\n",
      "Mismatches: 4\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0008130000\n",
      "Simulation error\n",
      "Mismatches: 4\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0007740000\n",
      "Simulation error\n",
      "Mismatches: 4\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0007920000\n",
      "Simulation error\n",
      "Mismatches: 4\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0007695000\n",
      "Response ranks: [0.625, 0.5, 0.5, 0.5, 0.5]\n",
      "Response lengths: [1066, 1177, 1049, 1091, 1044]\n",
      "\n",
      "FAILED. Compilation likely passed, but simulation failed.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Design a Verilog module named 'sequence_detector'.\n",
    "Ports:\n",
    " - Input: clk, rst_n (active low reset), in\n",
    " - Output: out\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Starting AutoChip Loop with Interface-Only Prompt: '{prompt}'\")\n",
    "\n",
    "final_response = verilog_loop(\n",
    "    design_prompt=prompt,\n",
    "    module=\"sequence_detector\",\n",
    "    testbench=\"sequence_detector_tb.v\",\n",
    "    max_iterations=4,\n",
    "    model_type=\"ChatGPT\",\n",
    "    model_id=\"gpt-3.5-turbo\",\n",
    "    outdir=output_dir,\n",
    "    log=\"generation_log_interfaces.txt\"\n",
    ")\n",
    "\n",
    "if final_response.rank == 1:\n",
    "    print(\"\\nSUCCESS! Verilog code generated and verified.\")\n",
    "else:\n",
    "    print(\"\\nFAILED. Compilation likely passed, but simulation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqQWnLWwU5jM",
    "outputId": "027bfd92-dc8d-46e9-b1c9-dc5b3ea47eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AutoChip Loop with ONE-SHOT Prompt...\n",
      "Iteration: 0\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Compilation error\n",
      "Cost for response 0: $0.0005880000\n",
      "Compilation error\n",
      "Cost for response 1: $0.0005715000\n",
      "Compilation error\n",
      "Cost for response 2: $0.0006750000\n",
      "Compilation error\n",
      "Cost for response 3: $0.0006180000\n",
      "Compilation error\n",
      "Cost for response 4: $0.0006465000\n",
      "Response ranks: [-1, -1, -1, -1, -1]\n",
      "Response lengths: [1376, 1194, 1536, 1198, 1457]\n",
      "Iteration: 1\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Compilation error\n",
      "Cost for response 0: $0.0010370000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0010535000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0010535000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0010790000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0010535000\n",
      "Response ranks: [-1, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1161, 1195, 1195, 1263, 1195]\n",
      "Iteration: 2\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0010635000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0010575000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0010575000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0010815000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0010575000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1201, 1195, 1195, 1266, 1195]\n",
      "Iteration: 3\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0010470000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0010575000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0010575000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0010665000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0010335000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1161, 1195, 1195, 1211, 1148]\n",
      "Iteration: 4\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 8\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0009975000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0010245000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0010245000\n",
      "Simulation error\n",
      "Mismatches: 8\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0009975000\n",
      "Response ranks: [0.0, 0.75, 0.75, 0.75, 0.0]\n",
      "Response lengths: [1126, 1148, 1210, 1210, 1128]\n",
      "Iteration: 5\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0010095000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1148, 1148, 1148, 1148, 1148]\n",
      "Iteration: 6\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0010155000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0010230000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0010095000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1148, 1148, 1175, 1190, 1148]\n",
      "Iteration: 7\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0010140000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0010245000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0010095000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1148, 1156, 1148, 1210, 1148]\n",
      "Iteration: 8\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0010065000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0010245000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0010110000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0010095000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1156, 1210, 1148, 1156, 1148]\n",
      "Iteration: 9\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0010320000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0010095000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1148, 1148, 1148, 1216, 1148]\n",
      "Iteration: 10\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-3.5-turbo\n",
      "Number of responses: 5\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 0: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 1: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 2: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 3: $0.0010095000\n",
      "Simulation error\n",
      "Mismatches: 2\n",
      "Samples: 8\n",
      "Cost for response 4: $0.0010095000\n",
      "Response ranks: [0.75, 0.75, 0.75, 0.75, 0.75]\n",
      "Response lengths: [1148, 1148, 1148, 1148, 1148]\n",
      "\n",
      "SUCCESS! \n",
      "------------------------------\n",
      "module sequence_detector (\n",
      "    input wire clk,\n",
      "    input wire rst_n,\n",
      "    input wire in,\n",
      "    output reg out\n",
      ");\n",
      "\n",
      "    // States\n",
      "    parameter S_IDLE = 2'b00;\n",
      "    parameter S_1 = 2'b01;\n",
      "    parameter S_10 = 2'b10;\n",
      "    parameter S_101 = 2'b11;\n",
      "    reg [1:0] state, next_state;\n",
      "\n",
      "    // Next state and output logic\n",
      "    always @(*) begin\n",
      "        next_state = state;\n",
      "        out = 1'b0;\n",
      "\n",
      "        case(state)\n",
      "            S_IDLE: begin\n",
      "                if(in == 1)\n",
      "                    next_state = S_1;\n",
      "            end\n",
      "            S_1: begin\n",
      "                if(in == 0)\n",
      "                    next_state = S_IDLE;\n",
      "                else\n",
      "                    next_state = S_10;\n",
      "            end\n",
      "            S_10: begin\n",
      "                if(in == 1)\n",
      "                    next_state = S_101;\n",
      "                else\n",
      "                    next_state = S_IDLE;\n",
      "            end\n",
      "            S_101: begin\n",
      "                out = 1'b1;\n",
      "                if(in == 0)\n",
      "                    next_state = S_IDLE;\n",
      "            end\n",
      "        endcase\n",
      "    end\n",
      "\n",
      "    // State transition\n",
      "    always @(posedge clk or negedge rst_n) begin\n",
      "        if(~rst_n)\n",
      "            state <= S_IDLE;\n",
      "        else\n",
      "            state <= next_state;\n",
      "    end\n",
      "\n",
      "endmodule\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a Verilog module named 'sequence_detector' to detect sequence '1011'.\n",
    "\n",
    "Constraint Checklist & Confidence Score:\n",
    "1. Detect Sequence? Yes.\n",
    "2. Machine Type? Mealy FSM.\n",
    "3. Output Logic? Combinational (assign out = ...).\n",
    "4. State Register? Yes.\n",
    "\n",
    "Mental Sandbox:\n",
    "- If I define a state S_1011, the output will be delayed by one clock cycle (Moore). The user wants Mealy.\n",
    "- Therefore, I MUST NOT define a state named S_1011 or S4.\n",
    "- I will only define states: S_IDLE, S_1, S_10, S_101.\n",
    "- When in S_101 and input is 1, output goes high immediately.\"\"\"\n",
    "\n",
    "print(f\"Starting AutoChip Loop with ONE-SHOT Prompt...\")\n",
    "\n",
    "final_response = verilog_loop(\n",
    "    design_prompt=prompt,\n",
    "    module=\"sequence_detector\",\n",
    "    testbench=\"sequence_detector_tb.v\",\n",
    "    max_iterations=10,\n",
    "    model_type=\"ChatGPT\",\n",
    "    model_id=\"gpt-3.5-turbo\",\n",
    "    outdir=output_dir,\n",
    "    log=\"generation_log_oneshot.txt\"\n",
    ")\n",
    "\n",
    "if final_response.rank >= 0:\n",
    "    print(\"\\nSUCCESS! \")\n",
    "    print(\"-\" * 30)\n",
    "    print(final_response.parsed_text)\n",
    "else:\n",
    "    print(\"\\nfailed\")\n",
    "    print(final_response.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF9mV0tFSDNj"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
