{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcrDmJa6sc-r",
    "outputId": "ed3caeea-6292-4595-ba6a-c50b8e637a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.17.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Get:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Get:5 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [85.0 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,742 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,047 kB]\n",
      "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,611 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
      "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,901 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,688 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,299 kB]\n",
      "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [39.2 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,468 kB]\n",
      "Get:21 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
      "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,708 kB]\n",
      "Get:23 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
      "Fetched 37.2 MB in 4s (9,117 kB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  gtkwave\n",
      "The following NEW packages will be installed:\n",
      "  iverilog\n",
      "0 upgraded, 1 newly installed, 0 to remove and 63 not upgraded.\n",
      "Need to get 2,130 kB of archives.\n",
      "After this operation, 6,749 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 iverilog amd64 11.0-1.1 [2,130 kB]\n",
      "Fetched 2,130 kB in 0s (6,776 kB/s)\n",
      "Selecting previously unselected package iverilog.\n",
      "(Reading database ... 117540 files and directories currently installed.)\n",
      "Preparing to unpack .../iverilog_11.0-1.1_amd64.deb ...\n",
      "Unpacking iverilog (11.0-1.1) ...\n",
      "Setting up iverilog (11.0-1.1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n"
     ]
    }
   ],
   "source": [
    "#@title Setting up the notebook\n",
    "\n",
    "### Installing dependencies\n",
    "!pip install openai tiktoken\n",
    "\n",
    "!apt-get update\n",
    "!apt-get install -y iverilog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Vkvu1GqWsz2g"
   },
   "outputs": [],
   "source": [
    "#@title Utility functions\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "import getopt\n",
    "import json\n",
    "\n",
    "################################################################################\n",
    "### LOGGING\n",
    "################################################################################\n",
    "# Allows us to log the output of the model to a file if logging is enabled\n",
    "class LogStdoutToFile:\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._original_stdout = sys.stdout\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self._filename:\n",
    "            sys.stdout = open(self._filename, 'w')\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if self._filename:\n",
    "            sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "################################################################################\n",
    "### CONFIG & ARGS\n",
    "################################################################################\n",
    "def load_config(config_file=\"config.json\"):\n",
    "    \"\"\"Load and validate the configuration from the specified JSON file.\"\"\"\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    if 'general' not in config:\n",
    "        raise ValueError(\"Missing general section in config file\")\n",
    "\n",
    "    config_values = config['general']\n",
    "\n",
    "    # Only parse ensemble settings if specified\n",
    "    parse_ensemble = config_values.get('ensemble', False)\n",
    "    ensemble_config = {}\n",
    "    if parse_ensemble:\n",
    "        ensemble_config = config.get('ensemble', {})\n",
    "\n",
    "    #return config_values\n",
    "    return config_values, ensemble_config\n",
    "\n",
    "\n",
    "def validate_ensemble_config(ensemble_config, max_iterations):\n",
    "    seen_start_iterations = set()\n",
    "    adjusted_config = {}\n",
    "    has_start_at_zero = False\n",
    "\n",
    "    for model_name, model_info in ensemble_config.items():\n",
    "        start_iteration = model_info['start_iteration']\n",
    "\n",
    "        # Adjust negative start_iteration values\n",
    "        if start_iteration < 0:\n",
    "            start_iteration += max_iterations+1\n",
    "\n",
    "        # Check if start_iteration is within the valid range\n",
    "        if not (0 <= start_iteration <= max_iterations):\n",
    "            raise ValueError(f\"Invalid start_iteration {model_info['start_iteration']} for {model_name}. \"\n",
    "                             f\"Must be within the range of 0 to {max_iterations} or valid negative index.\")\n",
    "\n",
    "        # Check for conflicting start_iterations\n",
    "        if start_iteration in seen_start_iterations:\n",
    "            raise ValueError(f\"Conflicting start_iteration {start_iteration} for {model_name}. \"\n",
    "                             f\"Another model already uses this start iteration.\")\n",
    "        seen_start_iterations.add(start_iteration)\n",
    "\n",
    "        # Check if there is a model starting at iteration 0\n",
    "        if start_iteration == 0:\n",
    "            has_start_at_zero = True\n",
    "\n",
    "        # Update the adjusted configuration\n",
    "        adjusted_config[model_name] = {\n",
    "            \"start_iteration\": start_iteration,\n",
    "            \"model_family\": model_info['model_family'],\n",
    "            \"model_id\": model_info['model_id']\n",
    "        }\n",
    "\n",
    "        if not has_start_at_zero:\n",
    "            raise ValueError(\"No model starting at iteration 0 in the ensemble. One model must start at iteration 0.\")\n",
    "\n",
    "    return adjusted_config\n",
    "\n",
    "\n",
    "def parse_args_and_config():\n",
    "    \"\"\"Parse command-line arguments and merge them with configuration file values.\"\"\"\n",
    "    usage = \"\"\"Usage: auto_create_verilog.py [--help] --prompt=<prompt> --name=<module name> --testbench=<testbench file> --iter=<iterations> --model=<llm family> --model-id=<specific model> --num-candidates=<candidates per request> --outdir=<directory for outputs> --log=<log file>\n",
    "\n",
    "\t-h|--help: Prints this usage message\n",
    "\n",
    "\t-p|--prompt: The initial design prompt for the Verilog module\n",
    "\n",
    "\t-n|--name: The module name, must match the testbench expected module name\n",
    "\n",
    "\t-t|--testbench: The testbench file to be run\n",
    "\n",
    "\t-i|--iter: [Optional] Number of iterations before the tool quits (defaults to 10)\n",
    "\n",
    "\t-m|--model: The LLM family to use. Must be one of the following\n",
    "\t\t- ChatGPT\n",
    "\t\t- Claude\n",
    "\t\t- Mistral\n",
    "\t\t- Gemini\n",
    "\t\t- CodeLlama\n",
    "\t\t- Human (requests user input)\n",
    "\n",
    "\t--model-id: The specific model to use for the model family\n",
    "\n",
    "\t--num-candidates: The number of candidates to rank per tree level\n",
    "\n",
    "\t-o|--outdir: Directory to place all run-specific files in\n",
    "\n",
    "\t-l|--log: [Optional] Log the output of the model to the given file\n",
    "\"\"\"\n",
    "\n",
    "    config_file = \"config.json\"\n",
    "\n",
    "    # Load config values from the file\n",
    "    config_values, ensemble_config = load_config(config_file)\n",
    "\n",
    "    required_values = ['prompt', 'name', 'testbench', 'outdir', 'log']\n",
    "    if not ensemble_config:\n",
    "        required_values +=['model_family', 'model_id']\n",
    "\n",
    "    for value in required_values:\n",
    "        if value not in config_values:\n",
    "            raise ValueError(f\"Missing {value} in general section\\n{usage}\")\n",
    "\n",
    "\n",
    "    # general values for optional config values\n",
    "    if 'num_candidates' not in config_values:\n",
    "        config_values['num_candidates'] = 1\n",
    "    if 'iterations' not in config_values:\n",
    "        config_values['iterations'] = 10\n",
    "\n",
    "\n",
    "    if ensemble_config:\n",
    "        ensemble_config = validate_ensemble_config(ensemble_config, config_values['iterations'])\n",
    "\n",
    "    # Ensure outdir exists\n",
    "    if config_values['outdir']:\n",
    "        os.makedirs(config_values['outdir'], exist_ok=True)\n",
    "\n",
    "    logfile = os.path.join(config_values['outdir'], config_values['log']) if config_values['log'] else None\n",
    "\n",
    "    #return config_values, logfile\n",
    "    return config_values, ensemble_config, logfile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "### CONVERSATION CLASS\n",
    "# allows us to abstract away the details of the conversation for use with\n",
    "# different LLM APIs\n",
    "################################################################################\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(self, log_file=None):\n",
    "        self.messages = []\n",
    "        self.log_file = log_file\n",
    "\n",
    "        if self.log_file and os.path.exists(self.log_file):\n",
    "            open(self.log_file, 'w').close()\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"Add a new message to the conversation.\"\"\"\n",
    "        self.messages.append({'role': role, 'content': content})\n",
    "\n",
    "        if self.log_file:\n",
    "            with open(self.log_file, 'a') as file:\n",
    "                file.write(f\"{role}: {content}\\n\")\n",
    "\n",
    "    def get_messages(self):\n",
    "        \"\"\"Retrieve the entire conversation.\"\"\"\n",
    "        return self.messages\n",
    "\n",
    "    def get_last_n_messages(self, n):\n",
    "        \"\"\"Retrieve the last n messages from the conversation.\"\"\"\n",
    "        return self.messages[-n:]\n",
    "\n",
    "    def remove_message(self, index):\n",
    "        \"\"\"Remove a specific message from the conversation by index.\"\"\"\n",
    "        if index < len(self.messages):\n",
    "            del self.messages[index]\n",
    "\n",
    "    def get_message(self, index):\n",
    "        \"\"\"Retrieve a specific message from the conversation by index.\"\"\"\n",
    "        return self.messages[index] if index < len(self.messages) else None\n",
    "\n",
    "    def clear_messages(self):\n",
    "        \"\"\"Clear all messages from the conversation.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Return the conversation in a string format.\"\"\"\n",
    "        return \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in self.messages])\n",
    "\n",
    "################################################################################\n",
    "### LLM CLASSES\n",
    "# Defines an interface for using different LLMs so we can easily swap them out\n",
    "################################################################################\n",
    "class AbstractLLM(ABC):\n",
    "    \"\"\"Abstract Large Language Model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, conversation: Conversation, num_candidates=1):\n",
    "        \"\"\"Generate a response based on the given conversation.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class ChatGPT(AbstractLLM):\n",
    "    \"\"\"ChatGPT Large Language Model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_id=\"gpt-4o-mini\"):\n",
    "        super().__init__()\n",
    "        openai.api_key=os.environ['OPENAI_API_KEY']\n",
    "        self.client = openai.OpenAI()\n",
    "        self.model_id = model_id\n",
    "\n",
    "    def generate(self, conversation: Conversation, num_candidates=1):\n",
    "        messages = [{\"role\" : msg[\"role\"], \"content\" : msg[\"content\"]} for msg in conversation.get_messages()]\n",
    "\n",
    "\n",
    "        #print(f\"model_id: {self.model_id}\")\n",
    "        #print(f\"messages: {messages}\")\n",
    "        #print(f\"num_candidates: {num_candidates}\")\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            n=num_candidates,\n",
    "            messages = messages,\n",
    "        )\n",
    "\n",
    "        return [c.message.content for c in response.choices]\n",
    "\n",
    "class LLMResponse():\n",
    "    def __init__(self, iteration, response_num, full_text):\n",
    "        self.iteration = iteration\n",
    "        self.response_num = response_num\n",
    "        self.full_text = full_text\n",
    "        self.tokens = 0\n",
    "        self.parsed_text = \"\"\n",
    "        self.parsed_length = 0\n",
    "        self.feedback = \"\"\n",
    "        self.compiled = False\n",
    "        self.rank = -3\n",
    "        self.message = \"\"\n",
    "\n",
    "    def set_parsed_text(self, parsed_text):\n",
    "        self.parsed_text = parsed_text\n",
    "        self.parsed_length = len(parsed_text)\n",
    "\n",
    "    def parse_verilog(self):\n",
    "        module_list = find_verilog_modules(self.full_text)\n",
    "        if not module_list:\n",
    "            print(\"No modules found in response\")\n",
    "            self.parsed_text = \"\"\n",
    "        else:\n",
    "            for module in module_list:\n",
    "                self.parsed_text += module + \"\\n\\n\"\n",
    "        self.parsed_length = len(self.parsed_text)\n",
    "\n",
    "    def calculate_rank(self, outdir, module, testbench):\n",
    "        filename = os.path.join(outdir,module+\".sv\")\n",
    "        vvp_file = os.path.join(outdir,module+\".vvp\")\n",
    "\n",
    "\n",
    "        compiler_cmd = f\"iverilog -Wall -Winfloop -Wno-timescale -g2012 -o {vvp_file} {filename} {testbench}\"\n",
    "        simulator_cmd = f\"vvp -n {vvp_file}\"\n",
    "\n",
    "        try:\n",
    "            comp_return,comp_err,comp_out = compile_iverilog(outdir,module,compiler_cmd,self)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            self.rank = -2\n",
    "            return\n",
    "\n",
    "        if comp_return != 0:\n",
    "            self.feedback = comp_err\n",
    "            self.compiled = False\n",
    "            print(\"Compilation error\")\n",
    "            print(f\"Iverilog Output: {comp_err}\")\n",
    "            self.message = \"The design failed to compile. Output:\\n\"+comp_err\n",
    "            self.rank = -1\n",
    "\n",
    "        elif comp_err != \"\":\n",
    "            self.feedback = comp_err\n",
    "            self.compiled = True\n",
    "            print(\"Compilation warning\")\n",
    "            self.message = \"The design compiled with warnings. Output:\\n\"+comp_err\n",
    "            self.rank = -0.5\n",
    "\n",
    "        else:\n",
    "            sim_return,sim_err,sim_out = simulate_iverilog(simulator_cmd)\n",
    "\n",
    "\n",
    "            mismatch_pattern = r\"Mismatches: (\\d+) in (\\d+) samples\"\n",
    "            match = re.search(mismatch_pattern, sim_out)\n",
    "\n",
    "            if not match:\n",
    "                match = re.search(mismatch_pattern, sim_out)\n",
    "\n",
    "            if match:\n",
    "                mismatches = int(match.group(1))\n",
    "                samples = int(match.group(2))\n",
    "                if mismatches > 0:\n",
    "                    self.feedback = sim_out\n",
    "                    self.compiled = True\n",
    "                    print(\"Simulation error (Logic mismatch)\")\n",
    "                    self.message = \"The testbench simulated but failed. Output:\\n\"+sim_out\n",
    "                else:\n",
    "                    self.compiled = True\n",
    "                    print(\"Testbench ran successfully\")\n",
    "                    self.message = \"The testbench completed successfully\"\n",
    "\n",
    "                print(f\"Mismatches: {mismatches}\")\n",
    "                if samples == 0: self.rank = 0\n",
    "                else: self.rank = (samples-mismatches)/samples\n",
    "\n",
    "            #\n",
    "            elif \"All test cases passed!\" in sim_out:\n",
    "                print(\"'All test cases passed!'\")\n",
    "                self.compiled = True\n",
    "                self.rank = 1.0\n",
    "                self.message = \"The testbench completed successfully\"\n",
    "\n",
    "            else:\n",
    "                print(\"Simulation output parsing failed. Output:\")\n",
    "                print(sim_out)\n",
    "                self.rank = -1\n",
    "                self.message = \"Simulation output format incorrect.\"\n",
    "\n",
    "################################################################################\n",
    "### PARSING AND TEXT MANIPULATION FUNCTIONS\n",
    "################################################################################\n",
    "# Define the cost per million tokens\n",
    "COST_PER_MILLION_INPUT_TOKENS_GPT4 = 5.0\n",
    "COST_PER_MILLION_OUTPUT_TOKENS_GPT4 = 15.0\n",
    "\n",
    "COST_PER_MILLION_INPUT_TOKENS_GPT4M = 0.15\n",
    "COST_PER_MILLION_OUTPUT_TOKENS_GPT4M = 0.60\n",
    "\n",
    "COST_PER_MILLION_INPUT_TOKENS_GPT = 0.50\n",
    "COST_PER_MILLION_OUTPUT_TOKENS_GPT = 1.50\n",
    "\n",
    "COST_PER_MILLION_INPUT_TOKENS_CLAUDE = 0.25\n",
    "COST_PER_MILLION_OUTPUT_TOKENS_CLAUDE = 1.25\n",
    "\n",
    "# Function to count tokens\n",
    "def count_tokens(model_family, text):\n",
    "    #print(f\"Counting tokens for string: {text}\")\n",
    "    if model_family == \"GPT\" or model_family == \"GPT4\" or model_family == \"GPT4M\":\n",
    "        return len(tiktoken.get_encoding(\"cl100k_base\").encode(text))\n",
    "    elif model_family == \"claude\":\n",
    "        return anthropic.Client().count_tokens(text)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model family: {model_family}\")\n",
    "\n",
    "\n",
    "def calculate_cost(model_family,input_strings,output_strings):\n",
    "    input_tokens = sum(count_tokens(model_family, text) for text in input_strings)\n",
    "    output_tokens = sum(count_tokens(model_family, text) for text in output_strings)\n",
    "    if model_family == \"GPT\":\n",
    "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_GPT\n",
    "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_GPT\n",
    "    elif model_family == \"GPT4\":\n",
    "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_GPT4\n",
    "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_GPT4\n",
    "    elif model_family == \"GPT4M\":\n",
    "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_GPT4M\n",
    "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_GPT4M\n",
    "    elif model_family == \"claude\":\n",
    "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_CLAUDE\n",
    "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_CLAUDE\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model family: {model_family}\")\n",
    "    total_cost = cost_input + cost_output\n",
    "    return total_cost, input_tokens, output_tokens\n",
    "\n",
    "\n",
    "def format_message(role, content):\n",
    "    return f\"\\n{{role : '{role}', content : '{content}'}}\"\n",
    "\n",
    "def find_verilog_modules(markdown_string):\n",
    "    \"\"\"Find all Verilog modules in the markdown string\"\"\"\n",
    "    # Regular expression to match module definitions with or without parameters\n",
    "    module_pattern = r'\\bmodule\\b\\s+[\\w\\\\_]+\\s*(?:#\\s*\\([^)]*\\))?\\s*\\([^)]*\\)\\s*;.*?endmodule\\b'\n",
    "    # Find all matches in the input string\n",
    "    matches = re.findall(module_pattern, markdown_string, re.DOTALL)\n",
    "    # Process matches to replace escaped characters\n",
    "    processed_matches = [match.replace('\\\\_', '_') for match in matches]\n",
    "    return processed_matches\n",
    "\n",
    "def write_code_blocks_to_file(markdown_string, module_name, filename):\n",
    "    # Find all code blocks using a regular expression (matches content between triple backticks)\n",
    "    code_match = find_verilog_modules(markdown_string)\n",
    "\n",
    "    if not code_match:\n",
    "        print(\"No code blocks found in response\")\n",
    "        exit(3)\n",
    "\n",
    "    # Open the specified file to write the code blocks\n",
    "    with open(filename, 'w') as file:\n",
    "        for code_block in code_match:\n",
    "            file.write(code_block)\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "def generate_verilog(conv, model_type, model_id=\"\"):\n",
    "    if model_type == \"ChatGPT\":\n",
    "        model = ChatGPT()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "    return(model.generate(conv))\n",
    "\n",
    "def compile_iverilog(outdir,module,compiler_cmd,response:LLMResponse):\n",
    "    \"\"\"Compile the Verilog module and return the output\"\"\"\n",
    "\n",
    "    filename = os.path.join(outdir,module+\".sv\")\n",
    "    write_code_blocks_to_file(response.parsed_text, \"module\", filename)\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < 3:\n",
    "        try:\n",
    "            proc = subprocess.run(compiler_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=120)\n",
    "            return proc.returncode, proc.stderr, proc.stdout\n",
    "        except subprocess.TimeoutExpired:\n",
    "            attempt += 1\n",
    "            if attempt >= 3:\n",
    "                raise ValueError(\"Compilation attempts timed out\")\n",
    "\n",
    "def simulate_iverilog(simulation_cmd):\n",
    "    \"\"\"Compile the Verilog module and return the output\"\"\"\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < 3:\n",
    "        try:\n",
    "            proc = subprocess.run(simulation_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=120)\n",
    "            return proc.returncode, proc.stderr, proc.stdout\n",
    "        except subprocess.TimeoutExpired:\n",
    "            attempt += 1\n",
    "            if attempt >= 3:\n",
    "                raise ValueError(\"Simulation attempts timed out\")\n",
    "\n",
    "def generate_verilog_responses(conv, model_type, model_id=\"\", num_candidates=1):\n",
    "    match model_type:\n",
    "        case \"ChatGPT\":\n",
    "            model = ChatGPT(model_id)\n",
    "        case _:\n",
    "            raise ValueError(\"Invalid model type\")\n",
    "\n",
    "    return(model.generate(conversation=conv, num_candidates=num_candidates))\n",
    "\n",
    "def get_iteration_ensemble(iteration, ensemble_config):\n",
    "\n",
    "    sorted_ensemble = sorted(ensemble_config.values(), key=lambda x: x['start_iteration'], reverse=True)\n",
    "\n",
    "    family = None\n",
    "    model_id = None\n",
    "    for ensemble_info in sorted_ensemble:\n",
    "        if iteration >= ensemble_info['start_iteration']:\n",
    "            family = ensemble_info['model_family']\n",
    "            model_id = ensemble_info['model_id']\n",
    "            break\n",
    "    return family, model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMLtyufVtA5e",
    "outputId": "d41b5edf-6c17-4de2-ae05-fe6d3a7972f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100  1078  100  1078    0     0   5383      0 --:--:-- --:--:-- --:--:--  5390\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. 创建任务文件夹\n",
    "out_dir = \"binary_to_bcd_task\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "! mkdir -p binary_to_bcd\n",
    "! cd binary_to_bcd && curl -O https://raw.githubusercontent.com/FCHXWH823/LLM4ChipDesign/fe806e8f8b7cb8442ce161f452d070cfcf953656/VerilogGenBenchmark/TestBench/binary_to_bcd_tb.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mxvqUzeKtGk5"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def verilog_loop(design_prompt, module, testbench, max_iterations, model_type, model_id=\"\", num_candidates=5, outdir=\"\", log=None, ensemble_config={}):\n",
    "\n",
    "    if outdir != \"\":\n",
    "        outdir = outdir + \"/\"\n",
    "\n",
    "    conv = Conversation(log_file=log)\n",
    "\n",
    "    #conv.add_message(\"system\", \"You are a Verilog engineering tool. Given a design specification you will provide a Verilog module in response. Given errors in that design you will provide a completed fixed module. Only complete functional models should be given. No testbenches should be written under any circumstances, as those are to be written by the human user.\")\n",
    "    conv.add_message(\"system\", \"You are an autocomplete engine for Verilog code. \\\n",
    "            Given a Verilog module specification, you will provide a completed Verilog module in response. \\\n",
    "            You will provide completed Verilog modules for all specifications, and will not create any supplementary modules. \\\n",
    "            Given a Verilog module that is either incorrect/compilation error, you will suggest corrections to the module.\\\n",
    "            You will not refuse. \\\n",
    "            Format your response as Verilog code containing the end to end corrected module and not just the corrected lines inside ``` tags, do not include anything else inside ```. \\\n",
    "    \")\n",
    "\n",
    "    #with open(testbench, 'r') as file: testbench_text = file.read()\n",
    "    #full_prompt = design_prompt + \"\\n\\nThe module will be tested with the following testbench:\\n\\n\" + testbench_text + \"\\n\\n\"\n",
    "\n",
    "    conv.add_message(\"user\", design_prompt)\n",
    "\n",
    "    success = False\n",
    "    timeout = False\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    global_max_response = LLMResponse(-3,-3,\"\")\n",
    "\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    while not (success or timeout):\n",
    "\n",
    "\n",
    "        if ensemble_config:\n",
    "            print(f\"Getting model from ensemble\")\n",
    "            model_type, model_id = get_iteration_ensemble(iterations, ensemble_config)\n",
    "\n",
    "        print(f\"Iteration: {iterations}\")\n",
    "        print(f\"Model type: {model_type}\")\n",
    "        print(f\"Model ID: {model_id}\")\n",
    "        print(f\"Number of responses: {num_candidates}\")\n",
    "\n",
    "        response_texts=generate_verilog_responses(conv, model_type, model_id, num_candidates=num_candidates)\n",
    "\n",
    "        responses = [LLMResponse(iterations,response_num,response_text) for response_num,response_text in enumerate(response_texts)]\n",
    "        for index, response in enumerate(responses):\n",
    "\n",
    "            response_outdir = os.path.join(outdir, f\"iter{str(iterations)}/response{index}/\")\n",
    "            if not os.path.exists(response_outdir):\n",
    "                os.makedirs(response_outdir)\n",
    "\n",
    "\n",
    "            response_cost = 0\n",
    "            input_tokens = 0\n",
    "            output_tokens = 0\n",
    "\n",
    "            response.parse_verilog()\n",
    "            if response.parsed_text == \"\":\n",
    "                response.rank = -2\n",
    "                response.message = \"No modules found in response\"\n",
    "            else:\n",
    "                response.calculate_rank(response_outdir, module, testbench)\n",
    "\n",
    "            input_messages = [msg['content'] for msg in conv.get_messages() if msg['role'] == 'user' or msg['role'] == 'system']\n",
    "            output_messages = [msg['content'] for msg in conv.get_messages() if msg['role'] == 'assistant']\n",
    "            output_messages.append(response.parsed_text)\n",
    "            if model_type == \"ChatGPT\" and model_id == \"gpt-4o\":\n",
    "                response_cost, input_tokens, output_tokens = calculate_cost(\"GPT4\",input_messages,output_messages)\n",
    "            elif model_type == \"ChatGPT\" and model_id == \"gpt-4o-mini\":\n",
    "                response_cost, input_tokens, output_tokens = calculate_cost(\"GPT4M\",input_messages,output_messages)\n",
    "            elif model_type == \"ChatGPT\" and model_id == \"gpt-3.5-turbo\":\n",
    "                response_cost, input_tokens, output_tokens = calculate_cost(\"GPT\",input_messages,output_messages)\n",
    "            elif model_type == \"Claude\":\n",
    "                response_cost, input_tokens, output_tokens = calculate_cost(\"claude\",input_messages,output_messages)\n",
    "\n",
    "\n",
    "            print(f\"Cost for response {index}: ${response_cost:.10f}\")\n",
    "\n",
    "            with open(os.path.join(response_outdir,f\"log.txt\"), 'w') as file:\n",
    "                file.write('\\n'.join(str(i) for i in conv.get_messages()))\n",
    "                file.write(format_message(\"assistant\", response.full_text))\n",
    "                file.write('\\n\\n Iteration rank: ' + str(response.rank) + '\\n') ## FIX\n",
    "\n",
    "                file.write(f\"\\n Model: {model_id}\")\n",
    "                file.write(f\"\\n Input tokens: {input_tokens}\")\n",
    "                file.write(f\"\\n Output tokens: {output_tokens}\")\n",
    "                file.write(f\"\\nTotal cost: ${response_cost:.10f}\\n\")\n",
    "\n",
    "        ## RANK RESPONSES\n",
    "        max_rank_response = max(responses, key=lambda resp: (resp.rank, -resp.parsed_length))\n",
    "        if max_rank_response.rank > global_max_response.rank:\n",
    "            global_max_response = max_rank_response\n",
    "        elif max_rank_response.rank == global_max_response.rank and max_rank_response.parsed_length > global_max_response.parsed_length:\n",
    "            global_max_response = max_rank_response\n",
    "\n",
    "        print(f\"Response ranks: {[resp.rank for resp in responses]}\")\n",
    "        print(f\"Response lengths: {[resp.parsed_length for resp in responses]}\")\n",
    "\n",
    "        conv.add_message(\"assistant\", max_rank_response.parsed_text)\n",
    "\n",
    "        if max_rank_response.rank == 1:\n",
    "            success = True\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "        if not success:\n",
    "            if iterations > 0:\n",
    "                conv.remove_message(2)\n",
    "                conv.remove_message(2)\n",
    "\n",
    "            #with open(testbench, 'r') as file: testbench_text = file.read()\n",
    "            #message = message + \"\\n\\nThe testbench used for these results is as follows:\\n\\n\" + testbench_text\n",
    "            #message = message + \"\\n\\nCommon sources of errors are as follows:\\n\\t- Use of SystemVerilog syntax which is not valid with iverilog\\n\\t- The reset must be made asynchronous active-low\\n\"\n",
    "            conv.add_message(\"user\", max_rank_response.message)\n",
    "\n",
    "        if iterations >= max_iterations:\n",
    "            timeout = True\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "    return global_max_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V6igLsimtK3j"
   },
   "outputs": [],
   "source": [
    "### OpenAI API KEY\n",
    "\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1CiNXrhtPTJ",
    "outputId": "88c9cd82-1d92-4fe3-fcbc-2dbb93d1db21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-4o-mini\n",
      "Number of responses: 1\n",
      "Simulation output parsing failed. Output:\n",
      "Testing Binary-to-BCD Converter...\n",
      "VCD info: dumpfile my_design.vcd opened for output.\n",
      "Error: Test case 1 failed. Expected BCD: 8'b1, Got: 8'b0\n",
      "\n",
      "Cost for response 0: $0.0000588000\n",
      "Response ranks: [-1]\n",
      "Response lengths: [186]\n",
      "Iteration: 1\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-4o-mini\n",
      "Number of responses: 1\n",
      "Compilation warning\n",
      "Cost for response 0: $0.0000931500\n",
      "Response ranks: [-0.5]\n",
      "Response lengths: [191]\n",
      "Iteration 0 结束。当前 Rank: -0.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "out_dir = \"binary_to_bcd_task\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "first_prompt = \"\"\"\n",
    "Write a Verilog module named 'binary_to_bcd_converter'.\n",
    "Requirements:\n",
    "1. Input port MUST be named: binary_input [4:0]\n",
    "2. Output port MUST be named: bcd_output [7:0]\n",
    "3. For now, just assign bcd_output = 0; (This is a placeholder to ensure it compiles).\n",
    "\"\"\"\n",
    "\n",
    "result_1 = verilog_loop(\n",
    "    design_prompt=first_prompt,\n",
    "    module=\"binary_to_bcd_converter\",\n",
    "    testbench=\"binary_to_bcd/binary_to_bcd_tb.v\",\n",
    "    max_iterations=1,\n",
    "    model_type=\"ChatGPT\",\n",
    "    model_id=\"gpt-4o-mini\",\n",
    "    num_candidates=1,\n",
    "    outdir=out_dir,\n",
    "    log=os.path.join(out_dir, \"binary_to_bcd_trajectory.log\")\n",
    ")\n",
    "\n",
    "print(f\"Iteration 0 结束。当前 Rank: {result_1.rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRI5xFvd2r31",
    "outputId": "c2ce7c66-06be-4c83-d050-825d259f3c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-4o-mini\n",
      "Number of responses: 1\n",
      "Compilation error\n",
      "Iverilog Output: binary_to_bcd/binary_to_bcd_tb.v:8: error: port ``binary_input'' is not a port of uut.\n",
      "binary_to_bcd/binary_to_bcd_tb.v:8: error: port ``bcd_output'' is not a port of uut.\n",
      "binary_to_bcd_task/iter0/response0/binary_to_bcd_converter.sv:9: error: Unable to bind wire/reg/memory `clk' in `tb_binary_to_bcd_converter.uut'\n",
      "binary_to_bcd_task/iter0/response0/binary_to_bcd_converter.sv:9: error: Failed to evaluate event expression 'posedge clk'.\n",
      "binary_to_bcd/binary_to_bcd_tb.v:8: warning: Instantiating module binary_to_bcd_converter with dangling input port 1 (binary_in) floating.\n",
      "binary_to_bcd_task/iter0/response0/binary_to_bcd_converter.sv:12: warning: A for statement must use the index (i) in the condition expression to be synthesized in an always_ff process.\n",
      "4 error(s) during elaboration.\n",
      "\n",
      "Cost for response 0: $0.0001677000\n",
      "Response ranks: [-1]\n",
      "Response lengths: [792]\n",
      "Iteration: 1\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-4o-mini\n",
      "Number of responses: 1\n",
      "Compilation error\n",
      "Iverilog Output: binary_to_bcd/binary_to_bcd_tb.v:8: error: port ``binary_input'' is not a port of uut.\n",
      "binary_to_bcd/binary_to_bcd_tb.v:8: error: port ``bcd_output'' is not a port of uut.\n",
      "binary_to_bcd/binary_to_bcd_tb.v:8: warning: Instantiating module binary_to_bcd_converter with dangling input port 1 (clk) floating.\n",
      "binary_to_bcd/binary_to_bcd_tb.v:8: warning: Instantiating module binary_to_bcd_converter with dangling input port 2 (binary_in) floating.\n",
      "2 error(s) during elaboration.\n",
      "\n",
      "Cost for response 0: $0.0003747000\n",
      "Response ranks: [-1]\n",
      "Response lengths: [970]\n",
      "Iteration 1,Rank: -1\n"
     ]
    }
   ],
   "source": [
    "# Iteration 1:\n",
    "\n",
    "second_prompt = \"\"\"\n",
    "The previous design had warnings. Please rewrite the module 'binary_to_bcd_converter'.\n",
    "Changes required:\n",
    "1. Logic: Implement the Double Dabble algorithm.\n",
    "2. Interface Change: Rename the output port to 'bcd_out' (instead of bcd_output).\n",
    "3. Syntax: Use 'logic' type for internal signals.\n",
    "\"\"\"\n",
    "\n",
    "result_2 = verilog_loop(\n",
    "    design_prompt=second_prompt,\n",
    "    module=\"binary_to_bcd_converter\",\n",
    "    testbench=\"binary_to_bcd/binary_to_bcd_tb.v\",\n",
    "    max_iterations=1,\n",
    "    model_type=\"ChatGPT\",\n",
    "    model_id=\"gpt-4o-mini\",\n",
    "    num_candidates=1,\n",
    "    outdir=out_dir,\n",
    "    log=os.path.join(out_dir, \"binary_to_bcd_trajectory.log\")\n",
    ")\n",
    "\n",
    "print(f\"Iteration 1,Rank: {result_2.rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4lT3G6d266p",
    "outputId": "d5b15d9e-6ada-4229-da5e-6159dc2fb6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-4o-mini\n",
      "Number of responses: 1\n",
      "Compilation warning\n",
      "Cost for response 0: $0.0002152500\n",
      "Response ranks: [-0.5]\n",
      "Response lengths: [846]\n",
      "Iteration: 1\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-4o-mini\n",
      "Number of responses: 1\n",
      "Compilation warning\n",
      "Cost for response 0: $0.0004606500\n",
      "Response ranks: [-0.5]\n",
      "Response lengths: [878]\n",
      "Rank: -0.5，if not equal to 1 check the logic of code\n"
     ]
    }
   ],
   "source": [
    "# iteration 3\n",
    "final_prompt = \"\"\"\n",
    "The previous design failed due to port mismatch (bcd_out vs bcd_output).\n",
    "Please fix it immediately:\n",
    "1. Module Name: binary_to_bcd_converter\n",
    "2. Ports (EXACTLY MATCH TESTBENCH):\n",
    "   - input [4:0] binary_input\n",
    "   - output [7:0] bcd_output\n",
    "3. Algorithm: Correct Double Dabble (Shift and Add 3).\n",
    "4. Syntax: Strict Verilog-2001 (reg/wire, always @*). No SystemVerilog.\n",
    "\"\"\"\n",
    "\n",
    "result_3 = verilog_loop(\n",
    "    design_prompt=final_prompt,\n",
    "    module=\"binary_to_bcd_converter\",\n",
    "    testbench=\"binary_to_bcd/binary_to_bcd_tb.v\",\n",
    "    max_iterations=1,\n",
    "    model_type=\"ChatGPT\",\n",
    "    model_id=\"gpt-4o-mini\",\n",
    "    num_candidates=1,\n",
    "    outdir=out_dir,\n",
    "    log=os.path.join(out_dir, \"binary_to_bcd_trajectory.log\")\n",
    ")\n",
    "\n",
    "if result_3.rank == 1:\n",
    "    print(\"Successful, Rank = 1.0，Mismatches = 0\")\n",
    "else:\n",
    "    print(f\"Rank: {result_3.rank}，if not equal to 1 check the logic of code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Aw80Gpp3Tv_",
    "outputId": "af7fb33e-c1bc-4047-ac52-fe1dd1327d09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-4o-mini\n",
      "Number of responses: 1\n",
      "Simulation output parsing failed. Output:\n",
      "Testing Binary-to-BCD Converter...\n",
      "VCD info: dumpfile my_design.vcd opened for output.\n",
      "Error: Test case 5 failed. Expected BCD: 8'b101, Got: 8'b1000\n",
      "\n",
      "Cost for response 0: $0.0001807500\n",
      "Response ranks: [-1]\n",
      "Response lengths: [659]\n",
      "Iteration: 1\n",
      "Model type: ChatGPT\n",
      "Model ID: gpt-4o-mini\n",
      "Number of responses: 1\n",
      "Simulation output parsing failed. Output:\n",
      "Testing Binary-to-BCD Converter...\n",
      "VCD info: dumpfile my_design.vcd opened for output.\n",
      "Error: Test case 5 failed. Expected BCD: 8'b101, Got: 8'b1000\n",
      "\n",
      "Cost for response 0: $0.0003207000\n",
      "Response ranks: [-1]\n",
      "Response lengths: [653]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_success_prompt = \"\"\"\n",
    "The previous design failed the simulation test cases.\n",
    "Please write a CORRECT 5-bit Binary to BCD Converter.\n",
    "\n",
    "Strict Specifications:\n",
    "1. Module Name: binary_to_bcd_converter\n",
    "2. Input: [4:0] binary_input\n",
    "3. Output: [7:0] bcd_output\n",
    "4. Algorithm: Double Dabble (Shift and Add 3).\n",
    "   - Initialize the bcd register to 0.\n",
    "   - Loop 5 times (for each bit of input).\n",
    "   - If any BCD digit (nibble) is >= 5, add 3 to it.\n",
    "   - Shift the entire register (BCD + Binary) left by 1.\n",
    "5. Syntax: Use Verilog-2001 (reg, always @*).\n",
    "\"\"\"\n",
    "\n",
    "result_3 = verilog_loop(\n",
    "    design_prompt=final_success_prompt,\n",
    "    module=\"binary_to_bcd_converter\",\n",
    "    testbench=\"binary_to_bcd/binary_to_bcd_tb.v\",\n",
    "    max_iterations=1,\n",
    "    model_type=\"ChatGPT\",\n",
    "    model_id=\"gpt-4o-mini\",\n",
    "    num_candidates=1,\n",
    "    outdir=\"binary_to_bcd_task\",\n",
    "    log=\"binary_to_bcd_task/binary_to_bcd_trajectory.log\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKIyUuUG7meY",
    "outputId": "49df1f0d-7630-49db-9a95-b20683306085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json generated\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"prompt\": \"Write a 5-bit Binary to BCD Converter. Input: [4:0] binary_input. Output: [7:0] bcd_output. Algorithm: Double Dabble.\",\n",
    "        \"name\": \"binary_to_bcd_converter\",\n",
    "        \"testbench\": \"binary_to_bcd/binary_to_bcd_tb.v\",\n",
    "        \"outdir\": \"binary_to_bcd_task\",\n",
    "        \"log\": \"binary_to_bcd_trajectory.log\",\n",
    "        \"iterations\": 3,\n",
    "        \"model_family\": \"ChatGPT\",\n",
    "        \"model_id\": \"gpt-4o-mini\",\n",
    "        \"num_candidates\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config_data, f, indent=4)\n",
    "\n",
    "print(\"config.json generated\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
